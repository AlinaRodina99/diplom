{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlinaRodina99/diplom/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF6qEmFHwbxV"
      },
      "outputs": [],
      "source": [
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeMV0gaTw2KT"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from skimage import io\n",
        "from PIL import Image, ImageFile\n",
        "import numpy as np\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt96TG23pfhs"
      },
      "outputs": [],
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sth9WZnO0QQM",
        "outputId": "03ac8fda-cfda-473b-d89c-ef443cd6e094"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff7ad65f690>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hDP83-76X_n"
      },
      "outputs": [],
      "source": [
        "class PaintingsAndPainters(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform):\n",
        "    self.annotations = pd.read_csv(csv_file, header=None)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    y_label = torch.tensor(self.annotations.iloc[index, 1])\n",
        "    img_path = self.root_dir + self.annotations.iloc[index, 0]\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    #transform = transforms.Grayscale()\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return (image, y_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTDn_6aQw4_h"
      },
      "outputs": [],
      "source": [
        "image_transforms = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YmAYgb9xUz1"
      },
      "outputs": [],
      "source": [
        "dataset = PaintingsAndPainters(csv_file= 'data_and_labels_new.csv', root_dir = '/content/drive/MyDrive/images_1/', transform = image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8LTGRDYE4mB"
      },
      "outputs": [],
      "source": [
        "m = len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geyNAxhs6BDn",
        "outputId": "74264b41-7241-499e-8ccd-6f314f2fec01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2023"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KofOtf70QKI7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcphx5BUEucE"
      },
      "outputs": [],
      "source": [
        "def mean_std(loader):\n",
        "  images, lebels = next(iter(loader))\n",
        "  # shape of images = [b,c,w,h]\n",
        "  mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
        "  return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1tPD62YeHgA"
      },
      "outputs": [],
      "source": [
        "new_image_transforms = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4489, 0.3996, 0.3405], [0.2815, 0.2766, 0.2672])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "5XwxLJsFeSeG",
        "outputId": "2d88dec9-e14a-4a4c-fdbd-aec31b276d7a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-96a98e045143>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnorm_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaintingsAndPainters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'data_and_labels.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/images_init/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_image_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'new_image_transforms' is not defined"
          ]
        }
      ],
      "source": [
        "norm_dataset = PaintingsAndPainters(csv_file= 'data_and_labels.csv', root_dir = '/content/drive/MyDrive/images_init/', transform = new_image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35_syvfHx-Vh"
      },
      "outputs": [],
      "source": [
        "generator_ = torch.Generator().manual_seed(17)\n",
        "train_data, val_data = random_split(dataset, [int(m-m*0.1), int(m*0.1) + 1], generator = generator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgZ8WzHqHRmb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB0qFYYzQGX3"
      },
      "outputs": [],
      "source": [
        "batch_size=8\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle = True, num_workers = 1)\n",
        "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle = False, num_workers = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hduRrkOntLUD"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhKDfAeHSgvQ"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdPVOi8kp51S"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.Upsample(scale_factor = 2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(200, 100, (5, 5)),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor = 2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(100, 3, (5, 5))\n",
        "        )\n",
        "\n",
        "        ### Convolutional section\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 100, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(100, 200, (5, 5)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2))\n",
        "        )\n",
        "        \n",
        "  def forward(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = x.view(-1, 744200)\n",
        "        x = self.decoder_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBqm6i2gHlwk"
      },
      "outputs": [],
      "source": [
        "class CNN(Autoencoder):\n",
        "  def __init__(self):\n",
        "    super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3muphNSIVOGS"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Un2ta-zETdm"
      },
      "outputs": [],
      "source": [
        "lr = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnypZR0ss4wU",
        "outputId": "dd09d33d-8b73-449e-abec-0922440feef3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (decoder_conv): Sequential(\n",
              "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (1): ReLU()\n",
              "    (2): ConvTranspose2d(200, 100, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (5): ReLU()\n",
              "    (6): ConvTranspose2d(100, 3, kernel_size=(5, 5), stride=(1, 1))\n",
              "  )\n",
              "  (encoder_conv): Sequential(\n",
              "    (0): Conv2d(3, 100, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(100, 200, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "autoencoder = Autoencoder()\n",
        "autoencoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LiVW_ySuqaa"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(autoencoder.parameters(), lr=lr, weight_decay = 1e-05)\n",
        "val_loss_prev = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHAF7DL2_6vB",
        "outputId": "5fab410c-dd10-4a53-a25d-b0836bfa5524"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008337477621301497"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/cnn_autoencoder.pt\")\n",
        "autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "lr = checkpoint['lr']\n",
        "counter = 1\n",
        "lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Nue3ZRGAOv"
      },
      "outputs": [],
      "source": [
        "autoencoder.decoder_conv = nn.Sequential(\n",
        "    nn.Linear(744200, 400),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(400, 200)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuxYcL6AiRVc"
      },
      "outputs": [],
      "source": [
        "autoencoder = autoencoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyWHMz84vVdY"
      },
      "outputs": [],
      "source": [
        "def add_noise(inputs,noise_factor=0.2):\n",
        "\tnoisy = inputs+torch.randn_like(inputs) * noise_factor\n",
        "\tnoisy = torch.clip(noisy,0.,1.)\n",
        "\treturn noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv62p6Tuu01I"
      },
      "outputs": [],
      "source": [
        "def train_epoch(autoencoder, device, dataloader, loss_fn, optimizer, epoch):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    train_loss = []\n",
        "    autoencoder.train()\n",
        "    avg_train_loss = 0\n",
        "    count = 0\n",
        "    global lr\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch, labels in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        # Move tensor to the proper device\n",
        "      #noisy_data = add_noise(image_batch).to(device)\n",
        "      #decoded_data = autoencoder(noisy_data)\n",
        "      outputs = autoencoder(image_batch)\n",
        "        # Evaluate loss\n",
        "      loss = loss_fn(outputs, labels)\n",
        "        # Backward pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        # Print batch loss\n",
        "      print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "      avg_train_loss += loss.item() / len(dataloader)\n",
        "      #torch.save({\n",
        "            #'model_state_dict': autoencoder.state_dict(),\n",
        "            #'optimizer_state_dict': optim.state_dict(),\n",
        "            #'lr' : lr\n",
        "            #}, \"/content/drive/MyDrive/new_new_model.pt\")\n",
        "    \n",
        "    lr *= 0.98\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr, weight_decay=1e-05)\n",
        "    print('ccc')\n",
        "    return avg_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjI-27CQvYqQ"
      },
      "outputs": [],
      "source": [
        "def test_epoch(autoencoder, device, dataloader, loss_fn):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    autoencoder.eval()\n",
        "    avg_val_loss = 0\n",
        "    count = 0\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch, labels in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            #if torch.cuda.is_available():\n",
        "               #print('yes')\n",
        "            image_batch.to(device)\n",
        "            outputs = autoencoder(image_batch)\n",
        "            # Decode data\n",
        "            #decoded_data = autoencoder(image_batch)\n",
        "            # Append the network output and the original image to the lists\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            avg_val_loss += loss.item() / len(dataloader)\n",
        "    return avg_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdaQpMnPNM9O"
      },
      "outputs": [],
      "source": [
        "val_losses = []\n",
        "val_accs = []\n",
        "def test_epoch(autoencoder, device, dataloader, loss_fn):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    autoencoder.eval()\n",
        "    avg_val_loss = 0\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch, labels in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            #if torch.cuda.is_available():\n",
        "               #print('yes')\n",
        "            image_batch = image_batch.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Decode data\n",
        "            outputs = autoencoder(image_batch)\n",
        "            # Append the network output and the original image to the lists\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            avg_val_loss += loss / len(dataloader)\n",
        "            predicted_labels.append(outputs.argmax(dim = 1))\n",
        "            real_labels.append(labels)\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    val_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    return avg_val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaCCNHPDGUhO"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrOsuw-4Pl_A"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u0bIDHkPIIJ"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "dataset = ConcatDataset([train_data, val_data])\n",
        "\n",
        "num_epochs=10\n",
        "batch_size=128\n",
        "k=10\n",
        "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
        "foldperf={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE9S8nrGQGek",
        "outputId": "c96faedc-ab45-4e72-b154-41f296133078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t partial train loss (single batch): 3.519453\n",
            "\t partial train loss (single batch): 3.507554\n",
            "\t partial train loss (single batch): 4.003544\n",
            "\t partial train loss (single batch): 5.206771\n",
            "\t partial train loss (single batch): 3.912924\n",
            "\t partial train loss (single batch): 4.937459\n",
            "\t partial train loss (single batch): 4.546063\n",
            "\t partial train loss (single batch): 3.993280\n",
            "\t partial train loss (single batch): 3.975260\n",
            "\t partial train loss (single batch): 3.959958\n",
            "\t partial train loss (single batch): 2.361553\n",
            "\t partial train loss (single batch): 2.991948\n",
            "\t partial train loss (single batch): 2.672703\n",
            "\t partial train loss (single batch): 3.887312\n",
            "\t partial train loss (single batch): 4.117747\n",
            "\t partial train loss (single batch): 3.946783\n",
            "\t partial train loss (single batch): 4.032081\n",
            "\t partial train loss (single batch): 4.616119\n",
            "\t partial train loss (single batch): 6.124329\n",
            "\t partial train loss (single batch): 4.841477\n",
            "\t partial train loss (single batch): 5.076682\n",
            "\t partial train loss (single batch): 4.731356\n",
            "\t partial train loss (single batch): 5.031735\n",
            "\t partial train loss (single batch): 4.938241\n",
            "\t partial train loss (single batch): 4.198451\n",
            "\t partial train loss (single batch): 4.318528\n",
            "\t partial train loss (single batch): 5.041721\n",
            "\t partial train loss (single batch): 4.090361\n",
            "\t partial train loss (single batch): 4.702106\n",
            "\t partial train loss (single batch): 4.010524\n",
            "\t partial train loss (single batch): 4.521994\n",
            "\t partial train loss (single batch): 5.049634\n",
            "\t partial train loss (single batch): 4.253656\n",
            "\t partial train loss (single batch): 3.774710\n",
            "\t partial train loss (single batch): 4.852512\n",
            "\t partial train loss (single batch): 3.608880\n",
            "\t partial train loss (single batch): 3.953543\n",
            "\t partial train loss (single batch): 4.776659\n",
            "\t partial train loss (single batch): 3.739974\n",
            "\t partial train loss (single batch): 3.176211\n",
            "\t partial train loss (single batch): 3.428341\n",
            "\t partial train loss (single batch): 4.230159\n",
            "\t partial train loss (single batch): 3.417669\n",
            "\t partial train loss (single batch): 3.647632\n",
            "\t partial train loss (single batch): 2.984566\n",
            "\t partial train loss (single batch): 3.734090\n",
            "\t partial train loss (single batch): 5.257043\n",
            "\t partial train loss (single batch): 4.226474\n",
            "\t partial train loss (single batch): 4.225501\n",
            "\t partial train loss (single batch): 5.006957\n",
            "\t partial train loss (single batch): 4.295920\n",
            "\t partial train loss (single batch): 4.905236\n",
            "\t partial train loss (single batch): 3.865659\n",
            "\t partial train loss (single batch): 3.802199\n",
            "\t partial train loss (single batch): 3.862425\n",
            "\t partial train loss (single batch): 4.390004\n",
            "\t partial train loss (single batch): 4.121128\n",
            "\t partial train loss (single batch): 4.934368\n",
            "\t partial train loss (single batch): 4.353879\n",
            "\t partial train loss (single batch): 3.495563\n",
            "\t partial train loss (single batch): 4.002090\n",
            "\t partial train loss (single batch): 3.210269\n",
            "\t partial train loss (single batch): 3.667141\n",
            "\t partial train loss (single batch): 4.480312\n",
            "\t partial train loss (single batch): 5.283486\n",
            "\t partial train loss (single batch): 3.747117\n",
            "\t partial train loss (single batch): 4.196207\n",
            "\t partial train loss (single batch): 3.954684\n",
            "\t partial train loss (single batch): 4.232671\n",
            "\t partial train loss (single batch): 4.889482\n",
            "\t partial train loss (single batch): 4.131852\n",
            "\t partial train loss (single batch): 3.138791\n",
            "\t partial train loss (single batch): 3.914865\n",
            "\t partial train loss (single batch): 3.781858\n",
            "\t partial train loss (single batch): 5.138278\n",
            "\t partial train loss (single batch): 4.068237\n",
            "\t partial train loss (single batch): 3.425182\n",
            "\t partial train loss (single batch): 4.446553\n",
            "\t partial train loss (single batch): 4.639278\n",
            "\t partial train loss (single batch): 4.941188\n",
            "\t partial train loss (single batch): 4.780515\n",
            "\t partial train loss (single batch): 4.924382\n",
            "\t partial train loss (single batch): 4.433030\n",
            "\t partial train loss (single batch): 4.025621\n",
            "\t partial train loss (single batch): 4.745150\n",
            "\t partial train loss (single batch): 6.219409\n",
            "\t partial train loss (single batch): 4.176445\n",
            "\t partial train loss (single batch): 4.165895\n",
            "\t partial train loss (single batch): 4.091714\n",
            "\t partial train loss (single batch): 4.845606\n",
            "\t partial train loss (single batch): 3.990628\n",
            "\t partial train loss (single batch): 4.596980\n",
            "\t partial train loss (single batch): 3.050859\n",
            "\t partial train loss (single batch): 3.706554\n",
            "\t partial train loss (single batch): 4.732858\n",
            "\t partial train loss (single batch): 3.821523\n",
            "\t partial train loss (single batch): 3.497588\n",
            "\t partial train loss (single batch): 3.326828\n",
            "\t partial train loss (single batch): 5.312568\n",
            "\t partial train loss (single batch): 4.881944\n",
            "\t partial train loss (single batch): 4.849554\n",
            "\t partial train loss (single batch): 6.145383\n",
            "\t partial train loss (single batch): 4.506757\n",
            "\t partial train loss (single batch): 4.658820\n",
            "\t partial train loss (single batch): 4.770680\n",
            "\t partial train loss (single batch): 3.644014\n",
            "\t partial train loss (single batch): 5.366996\n",
            "\t partial train loss (single batch): 4.571268\n",
            "\t partial train loss (single batch): 4.143952\n",
            "\t partial train loss (single batch): 4.624933\n",
            "\t partial train loss (single batch): 3.545065\n",
            "\t partial train loss (single batch): 4.023723\n",
            "\t partial train loss (single batch): 4.274909\n",
            "\t partial train loss (single batch): 4.317095\n",
            "\t partial train loss (single batch): 4.316391\n",
            "\t partial train loss (single batch): 3.323148\n",
            "\t partial train loss (single batch): 4.043321\n",
            "\t partial train loss (single batch): 4.285284\n",
            "\t partial train loss (single batch): 4.383643\n",
            "\t partial train loss (single batch): 3.540386\n",
            "\t partial train loss (single batch): 3.279556\n",
            "\t partial train loss (single batch): 4.095953\n",
            "\t partial train loss (single batch): 2.850617\n",
            "\t partial train loss (single batch): 3.595176\n",
            "\t partial train loss (single batch): 4.242335\n",
            "\t partial train loss (single batch): 2.644507\n",
            "\t partial train loss (single batch): 3.524797\n",
            "\t partial train loss (single batch): 5.912730\n",
            "\t partial train loss (single batch): 3.790093\n",
            "\t partial train loss (single batch): 3.495067\n",
            "\t partial train loss (single batch): 3.413041\n",
            "\t partial train loss (single batch): 4.688391\n",
            "\t partial train loss (single batch): 5.076755\n",
            "\t partial train loss (single batch): 2.863232\n",
            "\t partial train loss (single batch): 3.191848\n",
            "\t partial train loss (single batch): 4.028099\n",
            "\t partial train loss (single batch): 4.190441\n",
            "\t partial train loss (single batch): 4.056389\n",
            "\t partial train loss (single batch): 4.339673\n",
            "\t partial train loss (single batch): 5.436421\n",
            "\t partial train loss (single batch): 3.791184\n",
            "\t partial train loss (single batch): 4.083372\n",
            "\t partial train loss (single batch): 3.763652\n",
            "\t partial train loss (single batch): 4.004284\n",
            "\t partial train loss (single batch): 4.169166\n",
            "\t partial train loss (single batch): 4.948133\n",
            "\t partial train loss (single batch): 3.614079\n",
            "\t partial train loss (single batch): 4.262432\n",
            "\t partial train loss (single batch): 4.260940\n",
            "\t partial train loss (single batch): 4.525372\n",
            "\t partial train loss (single batch): 4.100779\n",
            "\t partial train loss (single batch): 3.524142\n",
            "\t partial train loss (single batch): 3.742321\n",
            "\t partial train loss (single batch): 3.041203\n",
            "\t partial train loss (single batch): 4.711516\n",
            "\t partial train loss (single batch): 4.132111\n",
            "\t partial train loss (single batch): 5.367222\n",
            "\t partial train loss (single batch): 4.917104\n",
            "\t partial train loss (single batch): 4.170480\n",
            "\t partial train loss (single batch): 4.623713\n",
            "\t partial train loss (single batch): 4.004393\n",
            "\t partial train loss (single batch): 4.347709\n",
            "\t partial train loss (single batch): 3.402084\n",
            "\t partial train loss (single batch): 3.851070\n",
            "\t partial train loss (single batch): 4.240109\n",
            "\t partial train loss (single batch): 4.703763\n",
            "\t partial train loss (single batch): 5.490570\n",
            "\t partial train loss (single batch): 4.062999\n",
            "\t partial train loss (single batch): 3.323305\n",
            "\t partial train loss (single batch): 5.051999\n",
            "\t partial train loss (single batch): 6.037557\n",
            "\t partial train loss (single batch): 4.379446\n",
            "\t partial train loss (single batch): 4.524786\n",
            "\t partial train loss (single batch): 4.929034\n",
            "\t partial train loss (single batch): 4.238064\n",
            "\t partial train loss (single batch): 3.728549\n",
            "\t partial train loss (single batch): 4.594932\n",
            "\t partial train loss (single batch): 4.937267\n",
            "\t partial train loss (single batch): 4.394790\n",
            "\t partial train loss (single batch): 4.555209\n",
            "\t partial train loss (single batch): 4.623715\n",
            "\t partial train loss (single batch): 4.177381\n",
            "\t partial train loss (single batch): 4.029631\n",
            "\t partial train loss (single batch): 3.861483\n",
            "\t partial train loss (single batch): 3.623299\n",
            "\t partial train loss (single batch): 5.145613\n",
            "\t partial train loss (single batch): 4.501612\n",
            "\t partial train loss (single batch): 4.639569\n",
            "\t partial train loss (single batch): 5.520697\n",
            "\t partial train loss (single batch): 4.571197\n",
            "\t partial train loss (single batch): 4.433013\n",
            "\t partial train loss (single batch): 4.478561\n",
            "\t partial train loss (single batch): 4.284988\n",
            "\t partial train loss (single batch): 5.454091\n",
            "\t partial train loss (single batch): 4.845402\n",
            "\t partial train loss (single batch): 5.442307\n",
            "\t partial train loss (single batch): 3.721360\n",
            "\t partial train loss (single batch): 4.282928\n",
            "\t partial train loss (single batch): 4.898119\n",
            "\t partial train loss (single batch): 4.907619\n",
            "\t partial train loss (single batch): 4.473631\n",
            "\t partial train loss (single batch): 4.452222\n",
            "\t partial train loss (single batch): 4.319260\n",
            "\t partial train loss (single batch): 4.445989\n",
            "\t partial train loss (single batch): 4.564736\n",
            "\t partial train loss (single batch): 4.633269\n",
            "\t partial train loss (single batch): 4.431663\n",
            "\t partial train loss (single batch): 3.464946\n",
            "\t partial train loss (single batch): 3.388040\n",
            "\t partial train loss (single batch): 6.170146\n",
            "\t partial train loss (single batch): 3.897621\n",
            "\t partial train loss (single batch): 3.994568\n",
            "\t partial train loss (single batch): 4.944468\n",
            "\t partial train loss (single batch): 3.960991\n",
            "\t partial train loss (single batch): 4.306678\n",
            "\t partial train loss (single batch): 5.119090\n",
            "\t partial train loss (single batch): 4.829093\n",
            "\t partial train loss (single batch): 4.451474\n",
            "\t partial train loss (single batch): 4.326710\n",
            "\t partial train loss (single batch): 4.601915\n",
            "\t partial train loss (single batch): 4.935420\n",
            "\t partial train loss (single batch): 4.049380\n",
            "\t partial train loss (single batch): 4.061313\n",
            "\t partial train loss (single batch): 4.076957\n",
            "\t partial train loss (single batch): 4.035349\n",
            "\t partial train loss (single batch): 4.068984\n",
            "\t partial train loss (single batch): 4.244281\n",
            "\t partial train loss (single batch): 3.513870\n",
            "ccc\n",
            "\n",
            " EPOCH 7/30 \t train loss 4.270211303443239 \t val loss (tensor(4.2106), tensor(0.1724))\n",
            "\t partial train loss (single batch): 4.503508\n",
            "\t partial train loss (single batch): 3.922157\n",
            "\t partial train loss (single batch): 3.842531\n",
            "\t partial train loss (single batch): 5.200692\n",
            "\t partial train loss (single batch): 3.814987\n",
            "\t partial train loss (single batch): 4.351157\n",
            "\t partial train loss (single batch): 4.365136\n",
            "\t partial train loss (single batch): 2.776941\n",
            "\t partial train loss (single batch): 4.177786\n",
            "\t partial train loss (single batch): 4.620646\n",
            "\t partial train loss (single batch): 5.046463\n",
            "\t partial train loss (single batch): 4.011794\n",
            "\t partial train loss (single batch): 4.681199\n",
            "\t partial train loss (single batch): 3.366666\n",
            "\t partial train loss (single batch): 2.934245\n",
            "\t partial train loss (single batch): 4.068079\n",
            "\t partial train loss (single batch): 3.616647\n",
            "\t partial train loss (single batch): 4.017964\n",
            "\t partial train loss (single batch): 3.291186\n",
            "\t partial train loss (single batch): 5.229258\n",
            "\t partial train loss (single batch): 4.421455\n",
            "\t partial train loss (single batch): 4.084044\n",
            "\t partial train loss (single batch): 3.450129\n",
            "\t partial train loss (single batch): 3.494899\n",
            "\t partial train loss (single batch): 4.443651\n",
            "\t partial train loss (single batch): 3.152237\n",
            "\t partial train loss (single batch): 3.997162\n",
            "\t partial train loss (single batch): 4.097567\n",
            "\t partial train loss (single batch): 4.868223\n",
            "\t partial train loss (single batch): 3.382677\n",
            "\t partial train loss (single batch): 5.332713\n",
            "\t partial train loss (single batch): 4.225539\n",
            "\t partial train loss (single batch): 4.540983\n",
            "\t partial train loss (single batch): 4.066815\n",
            "\t partial train loss (single batch): 5.195329\n",
            "\t partial train loss (single batch): 3.835114\n",
            "\t partial train loss (single batch): 4.497282\n",
            "\t partial train loss (single batch): 4.233284\n",
            "\t partial train loss (single batch): 4.722908\n",
            "\t partial train loss (single batch): 3.683208\n",
            "\t partial train loss (single batch): 4.233422\n",
            "\t partial train loss (single batch): 4.596954\n",
            "\t partial train loss (single batch): 4.185621\n",
            "\t partial train loss (single batch): 3.976176\n",
            "\t partial train loss (single batch): 4.458313\n",
            "\t partial train loss (single batch): 3.735601\n",
            "\t partial train loss (single batch): 5.019689\n",
            "\t partial train loss (single batch): 4.134632\n",
            "\t partial train loss (single batch): 3.381488\n",
            "\t partial train loss (single batch): 4.627428\n",
            "\t partial train loss (single batch): 4.350130\n",
            "\t partial train loss (single batch): 4.819325\n",
            "\t partial train loss (single batch): 3.357133\n",
            "\t partial train loss (single batch): 5.276698\n",
            "\t partial train loss (single batch): 6.099751\n",
            "\t partial train loss (single batch): 3.678609\n",
            "\t partial train loss (single batch): 4.592066\n",
            "\t partial train loss (single batch): 4.525916\n",
            "\t partial train loss (single batch): 4.917408\n",
            "\t partial train loss (single batch): 4.711572\n",
            "\t partial train loss (single batch): 4.442639\n",
            "\t partial train loss (single batch): 4.832173\n",
            "\t partial train loss (single batch): 4.406396\n",
            "\t partial train loss (single batch): 4.342436\n",
            "\t partial train loss (single batch): 4.890686\n",
            "\t partial train loss (single batch): 3.755626\n",
            "\t partial train loss (single batch): 4.242598\n",
            "\t partial train loss (single batch): 4.217920\n",
            "\t partial train loss (single batch): 3.738294\n",
            "\t partial train loss (single batch): 3.911096\n",
            "\t partial train loss (single batch): 4.894317\n",
            "\t partial train loss (single batch): 4.120754\n",
            "\t partial train loss (single batch): 4.790405\n",
            "\t partial train loss (single batch): 4.138134\n",
            "\t partial train loss (single batch): 2.673677\n",
            "\t partial train loss (single batch): 3.710267\n",
            "\t partial train loss (single batch): 3.998596\n",
            "\t partial train loss (single batch): 4.525588\n",
            "\t partial train loss (single batch): 2.785883\n",
            "\t partial train loss (single batch): 3.060101\n",
            "\t partial train loss (single batch): 6.088490\n",
            "\t partial train loss (single batch): 3.000451\n",
            "\t partial train loss (single batch): 3.156459\n",
            "\t partial train loss (single batch): 4.026257\n",
            "\t partial train loss (single batch): 5.745636\n",
            "\t partial train loss (single batch): 4.019408\n",
            "\t partial train loss (single batch): 4.833803\n",
            "\t partial train loss (single batch): 4.483479\n",
            "\t partial train loss (single batch): 5.367965\n",
            "\t partial train loss (single batch): 4.032510\n",
            "\t partial train loss (single batch): 3.076182\n",
            "\t partial train loss (single batch): 4.028076\n",
            "\t partial train loss (single batch): 4.625504\n",
            "\t partial train loss (single batch): 3.311772\n",
            "\t partial train loss (single batch): 4.397967\n",
            "\t partial train loss (single batch): 3.910462\n",
            "\t partial train loss (single batch): 3.650968\n",
            "\t partial train loss (single batch): 4.674278\n",
            "\t partial train loss (single batch): 3.442569\n",
            "\t partial train loss (single batch): 4.312878\n",
            "\t partial train loss (single batch): 3.229530\n",
            "\t partial train loss (single batch): 4.449870\n",
            "\t partial train loss (single batch): 4.897058\n",
            "\t partial train loss (single batch): 4.707887\n",
            "\t partial train loss (single batch): 4.285180\n",
            "\t partial train loss (single batch): 4.101614\n",
            "\t partial train loss (single batch): 3.879055\n",
            "\t partial train loss (single batch): 4.647158\n",
            "\t partial train loss (single batch): 4.273781\n",
            "\t partial train loss (single batch): 3.616236\n",
            "\t partial train loss (single batch): 5.450015\n",
            "\t partial train loss (single batch): 4.200231\n",
            "\t partial train loss (single batch): 2.786445\n",
            "\t partial train loss (single batch): 4.939381\n",
            "\t partial train loss (single batch): 4.446996\n",
            "\t partial train loss (single batch): 3.684733\n",
            "\t partial train loss (single batch): 4.296054\n",
            "\t partial train loss (single batch): 3.321980\n",
            "\t partial train loss (single batch): 2.247227\n",
            "\t partial train loss (single batch): 3.515173\n",
            "\t partial train loss (single batch): 6.467432\n",
            "\t partial train loss (single batch): 3.695752\n",
            "\t partial train loss (single batch): 5.585951\n",
            "\t partial train loss (single batch): 3.479815\n",
            "\t partial train loss (single batch): 3.715034\n",
            "\t partial train loss (single batch): 5.323176\n",
            "\t partial train loss (single batch): 4.734510\n",
            "\t partial train loss (single batch): 4.093137\n",
            "\t partial train loss (single batch): 4.615526\n",
            "\t partial train loss (single batch): 5.257051\n",
            "\t partial train loss (single batch): 4.441846\n",
            "\t partial train loss (single batch): 4.945734\n",
            "\t partial train loss (single batch): 5.034702\n",
            "\t partial train loss (single batch): 4.185963\n",
            "\t partial train loss (single batch): 4.035700\n",
            "\t partial train loss (single batch): 4.020418\n",
            "\t partial train loss (single batch): 4.325787\n",
            "\t partial train loss (single batch): 4.716798\n",
            "\t partial train loss (single batch): 4.208961\n",
            "\t partial train loss (single batch): 5.316800\n",
            "\t partial train loss (single batch): 4.904737\n",
            "\t partial train loss (single batch): 5.418284\n",
            "\t partial train loss (single batch): 4.129643\n",
            "\t partial train loss (single batch): 4.316495\n",
            "\t partial train loss (single batch): 4.331192\n",
            "\t partial train loss (single batch): 4.197473\n",
            "\t partial train loss (single batch): 2.965207\n",
            "\t partial train loss (single batch): 3.563699\n",
            "\t partial train loss (single batch): 4.993904\n",
            "\t partial train loss (single batch): 4.768915\n",
            "\t partial train loss (single batch): 4.293078\n",
            "\t partial train loss (single batch): 4.271928\n",
            "\t partial train loss (single batch): 4.026570\n",
            "\t partial train loss (single batch): 3.793862\n",
            "\t partial train loss (single batch): 4.032438\n",
            "\t partial train loss (single batch): 5.585094\n",
            "\t partial train loss (single batch): 4.225077\n",
            "\t partial train loss (single batch): 4.269290\n",
            "\t partial train loss (single batch): 3.367872\n",
            "\t partial train loss (single batch): 4.613336\n",
            "\t partial train loss (single batch): 3.368198\n",
            "\t partial train loss (single batch): 5.078842\n",
            "\t partial train loss (single batch): 2.356726\n",
            "\t partial train loss (single batch): 4.429775\n",
            "\t partial train loss (single batch): 4.537135\n",
            "\t partial train loss (single batch): 4.054565\n",
            "\t partial train loss (single batch): 4.675513\n",
            "\t partial train loss (single batch): 4.082125\n",
            "\t partial train loss (single batch): 3.979817\n",
            "\t partial train loss (single batch): 5.610556\n",
            "\t partial train loss (single batch): 3.545000\n",
            "\t partial train loss (single batch): 6.000815\n",
            "\t partial train loss (single batch): 4.059949\n",
            "\t partial train loss (single batch): 3.358568\n",
            "\t partial train loss (single batch): 3.944402\n",
            "\t partial train loss (single batch): 3.640283\n",
            "\t partial train loss (single batch): 4.090621\n",
            "\t partial train loss (single batch): 4.946245\n",
            "\t partial train loss (single batch): 3.985018\n",
            "\t partial train loss (single batch): 4.287881\n",
            "\t partial train loss (single batch): 4.587307\n",
            "\t partial train loss (single batch): 3.458878\n",
            "\t partial train loss (single batch): 4.580550\n",
            "\t partial train loss (single batch): 4.827284\n",
            "\t partial train loss (single batch): 4.394978\n",
            "\t partial train loss (single batch): 5.096060\n",
            "\t partial train loss (single batch): 3.226339\n",
            "\t partial train loss (single batch): 4.418938\n",
            "\t partial train loss (single batch): 3.559735\n",
            "\t partial train loss (single batch): 4.053658\n",
            "\t partial train loss (single batch): 3.286531\n",
            "\t partial train loss (single batch): 4.566720\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#for epoch in range(4, num_epochs):\n",
        "#3\n",
        "num_epochs = 30\n",
        "for epoch in range(6, num_epochs):\n",
        "      train_loss =train_epoch(autoencoder,device, train_loader,loss_fn, optim, epoch)\n",
        "      val_loss = test_epoch(autoencoder, device,valid_loader,loss_fn)\n",
        "      torch.save({\n",
        "            'model_state_dict': autoencoder.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'loss': val_loss,\n",
        "            'counter': counter,\n",
        "            #'scheduler': scheduler,\n",
        "            'lr'\n",
        "            \n",
        "             : lr,\n",
        "            'epoch': epoch\n",
        "            }, \"/content/drive/MyDrive/cnn_autoencoder.pt\")\n",
        "      #if epoch > 4:\n",
        "      \n",
        "        #val_loss_prev = checkpoint['loss']\n",
        "        #if val_loss_prev - val_loss <= 0:\n",
        "           #counter += 1\n",
        "           #if counter > 5:\n",
        "            # break\n",
        "      print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "StLDMjq-K22K",
        "outputId": "e45a2b4c-9bf4-4afe-c9a8-b34271f86760"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e2ab3ea573f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "inu4g_7CvlxG",
        "outputId": "c69497e0-4141-4597-f5b6-bbeece1ac9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t partial train loss (single batch): 0.051994\n",
            "\t partial train loss (single batch): 0.106104\n",
            "\t partial train loss (single batch): 0.072035\n",
            "\t partial train loss (single batch): 0.074467\n",
            "\t partial train loss (single batch): 0.062407\n",
            "\t partial train loss (single batch): 0.069505\n",
            "\t partial train loss (single batch): 0.080621\n",
            "\t partial train loss (single batch): 0.062709\n",
            "\t partial train loss (single batch): 0.111110\n",
            "\t partial train loss (single batch): 0.081672\n",
            "\t partial train loss (single batch): 0.075219\n",
            "\t partial train loss (single batch): 0.091972\n",
            "\t partial train loss (single batch): 0.091051\n",
            "\t partial train loss (single batch): 0.068592\n",
            "\t partial train loss (single batch): 0.063609\n",
            "\t partial train loss (single batch): 0.076215\n",
            "\t partial train loss (single batch): 0.071613\n",
            "\t partial train loss (single batch): 0.053021\n",
            "\t partial train loss (single batch): 0.077674\n",
            "\t partial train loss (single batch): 0.089189\n",
            "\t partial train loss (single batch): 0.066067\n",
            "\t partial train loss (single batch): 0.078061\n",
            "\t partial train loss (single batch): 0.084668\n",
            "\t partial train loss (single batch): 0.073368\n",
            "\t partial train loss (single batch): 0.073120\n",
            "\t partial train loss (single batch): 0.065198\n",
            "\t partial train loss (single batch): 0.043388\n",
            "\t partial train loss (single batch): 0.062489\n",
            "\t partial train loss (single batch): 0.063680\n",
            "\t partial train loss (single batch): 0.095402\n",
            "\t partial train loss (single batch): 0.091761\n",
            "\t partial train loss (single batch): 0.069889\n",
            "\t partial train loss (single batch): 0.071314\n",
            "\t partial train loss (single batch): 0.102587\n",
            "\t partial train loss (single batch): 0.070070\n",
            "\t partial train loss (single batch): 0.050970\n",
            "\t partial train loss (single batch): 0.080959\n",
            "\t partial train loss (single batch): 0.066807\n",
            "\t partial train loss (single batch): 0.052648\n",
            "\t partial train loss (single batch): 0.088650\n",
            "\t partial train loss (single batch): 0.059912\n",
            "\t partial train loss (single batch): 0.098191\n",
            "\t partial train loss (single batch): 0.073464\n",
            "\t partial train loss (single batch): 0.072380\n",
            "\t partial train loss (single batch): 0.077291\n",
            "\t partial train loss (single batch): 0.069381\n",
            "\t partial train loss (single batch): 0.051390\n",
            "\t partial train loss (single batch): 0.074166\n",
            "\t partial train loss (single batch): 0.062277\n",
            "\t partial train loss (single batch): 0.069971\n",
            "\t partial train loss (single batch): 0.104854\n",
            "\t partial train loss (single batch): 0.102952\n",
            "\t partial train loss (single batch): 0.086945\n",
            "\t partial train loss (single batch): 0.079374\n",
            "\t partial train loss (single batch): 0.054088\n",
            "\t partial train loss (single batch): 0.071215\n",
            "\t partial train loss (single batch): 0.079890\n",
            "\t partial train loss (single batch): 0.083607\n",
            "\t partial train loss (single batch): 0.074537\n",
            "\t partial train loss (single batch): 0.068630\n",
            "\t partial train loss (single batch): 0.077615\n",
            "\t partial train loss (single batch): 0.051938\n",
            "\t partial train loss (single batch): 0.064082\n",
            "\t partial train loss (single batch): 0.092045\n",
            "\t partial train loss (single batch): 0.061856\n",
            "\t partial train loss (single batch): 0.045298\n",
            "\t partial train loss (single batch): 0.099048\n",
            "\t partial train loss (single batch): 0.108174\n",
            "\t partial train loss (single batch): 0.078780\n",
            "\t partial train loss (single batch): 0.064657\n",
            "\t partial train loss (single batch): 0.049369\n",
            "\t partial train loss (single batch): 0.061007\n",
            "\t partial train loss (single batch): 0.054083\n",
            "\t partial train loss (single batch): 0.071550\n",
            "\t partial train loss (single batch): 0.085872\n",
            "\t partial train loss (single batch): 0.073608\n",
            "\t partial train loss (single batch): 0.087348\n",
            "\t partial train loss (single batch): 0.063852\n",
            "\t partial train loss (single batch): 0.094637\n",
            "\t partial train loss (single batch): 0.062447\n",
            "\t partial train loss (single batch): 0.061672\n",
            "\t partial train loss (single batch): 0.067558\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "#for epoch in range(4, num_epochs):\n",
        "#3\n",
        "for epoch in range(4, num_epochs):\n",
        "   train_loss =train_epoch(autoencoder,device, train_loader,loss_fn, optim, epoch)\n",
        "   val_loss = test_epoch(autoencoder, device,valid_loader,loss_fn)\n",
        "   torch.save({\n",
        "            'model_state_dict': autoencoder.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'loss': val_loss,\n",
        "            'counter': counter,\n",
        "            #'scheduler': scheduler,\n",
        "            'lr' : lr,\n",
        "            'epoch': epoch\n",
        "            }, \"/content/drive/MyDrive/model.pt\")\n",
        "   if epoch > 4:\n",
        "      \n",
        "     val_loss_prev = checkpoint['loss']\n",
        "     if val_loss_prev - val_loss <= 0:\n",
        "       counter += 1\n",
        "       if counter > 5:\n",
        "         break\n",
        "   print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1w6ZxljhgaUutIoEcsjIgKNtkdaJXecLt",
      "authorship_tag": "ABX9TyMeO7He/JicB/Vj0/rN+rBm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}